keras_gpu_options:
  allocator_type: '' # string allocator_type
  deferred_deletion_bytes: 0 # int64 deferred_deletion_bytes
  experimental: '' # Experimental experimental
  force_gpu_compatible: True # bool force_gpu_compatible
  per_process_gpu_memory_fraction: 0.0 # double per_process_gpu_memory_fraction
  polling_active_delay_usecs: 0 # int32 polling_active_delay_usecs
  polling_inactive_delay_msecs: 0 # int32 polling_inactive_delay_msecs
  visible_device_list: '' # string visible_device_list
keras_layers_Activation:
  activation: enumerate(('deserialize', 'elu', 'exponential', 'get', 'hard_sigmoid', 'linear', 'relu', 'selu', 'serialize', 'sigmoid', 'softmax', 'softplus', 'softsign', 'swish', 'tanh')) # Activation function to use. If you don't specify anything, no activation is applied (see keras.activations).
  name: '' #  
keras_layers_Input:
  batch_size: 0 # optional static batch size (integer).
  name: '' # An optional name string for the layer. Should be unique in a model (do not reuse the same name twice). It will be autogenerated if it isn't provided.
  dtype: 'float32' # The data type expected by the input, as a string (float32, float64, int32...)
  sparse: False # A boolean specifying whether the placeholder to be created is sparse. Only one of 'ragged' and 'sparse' can be True. Note that, if sparse is False, sparse tensors can still be passed into the input - they will be densified with a default value of 0.
  tensor: [[0.0]] # Optional existing tensor to wrap into the Input layer. If set, the layer will use the tf.TypeSpec of this tensor rather than creating a new placeholder tensor.
  ragged: False # A boolean specifying whether the placeholder to be created is ragged. Only one of 'ragged' and 'sparse' can be True. In this case, values of 'None' in the 'shape' argument represent ragged dimensions. For more information about RaggedTensors, see this guide.
  type_spec: [[0.0]] # A tf.TypeSpec object to create the input placeholder from. When provided, all other args except name must be None.
keras_layers_Conv3D:
  strides: (1,1,1) # An integer or tuple/list of 3 integers, specifying the strides of the convolution along each spatial dimension. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1.
  padding: enumerate(('valid', 'same')) # one of "valid" or "same" (case-insensitive). "valid" means no padding. "same" results in padding with zeros evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input.
  data_format: '' # A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape batch_shape + (spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape batch_shape + (channels, spatial_dim1, spatial_dim2, spatial_dim3). It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json. If you never set it, then it will be "channels_last".
  dilation_rate: (1,1,1) # an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1.
  groups: 0 # A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups.
  activation: enumerate(('deserialize', 'elu', 'exponential', 'get', 'hard_sigmoid', 'linear', 'relu', 'selu', 'serialize', 'sigmoid', 'softmax', 'softplus', 'softsign', 'swish', 'tanh')) # Activation function to use. If you don't specify anything, no activation is applied (see keras.activations).
  use_bias: False # Boolean, whether the layer uses a bias vector.
  kernel_initializer: enumerate(('Constant', 'GlorotNormal', 'GlorotUniform', 'HeNormal', 'HeUniform', 'Identity', 'Initializer', 'LecunNormal', 'LecunUniform', 'Ones', 'Orthogonal', 'RandomNormal', 'RandomUniform', 'TruncatedNormal', 'VarianceScaling', 'Zeros', 'constant', 'deserialize', 'get', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform', 'identity', 'lecun_normal', 'lecun_uniform', 'ones', 'orthogonal', 'random_normal', 'random_uniform', 'serialize', 'truncated_normal', 'variance_scaling', 'zeros')) # Initializer for the kernel weights matrix (see keras.initializers). Defaults to 'glorot_uniform'.# 
  bias_initializer: enumerate(('Constant', 'GlorotNormal', 'GlorotUniform', 'HeNormal', 'HeUniform', 'Identity', 'Initializer', 'LecunNormal', 'LecunUniform', 'Ones', 'Orthogonal', 'RandomNormal', 'RandomUniform', 'TruncatedNormal', 'VarianceScaling', 'Zeros', 'constant', 'deserialize', 'get', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform', 'identity', 'lecun_normal', 'lecun_uniform', 'ones', 'orthogonal', 'random_normal', 'random_uniform', 'serialize', 'truncated_normal', 'variance_scaling', 'zeros')) # Initializer for the bias vector (see keras.initializers). Defaults to 'zeros'.
  kernel_regularizer: enumerate(('L1', 'L1L2', 'L2', 'Regularizer', 'get', 'l1', 'l1_l2', 'l2', 'serialize')) # Regularizer function applied to the kernel weights matrix (see keras.regularizers).
  bias_regularizer: enumerate(('L1', 'L1L2', 'L2', 'Regularizer', 'get', 'l1', 'l1_l2', 'l2', 'serialize')) # Regularizer function applied to the bias vector (see keras.regularizers).
  activity_regularizer: enumerate(('L1', 'L1L2', 'L2', 'Regularizer', 'get', 'l1', 'l1_l2', 'l2', 'serialize')) # Regularizer function applied to the output of the layer (its "activation") (see keras.regularizers).
  kernel_constraint: enumerate(('Constraint', 'MaxNorm', 'MinMaxNorm', 'NonNeg', 'RadialConstraint', 'UnitNorm','deserialize', 'get', 'max_norm', 'min_max_norm', 'non_neg', 'radial_constraint', 'serialize', 'unit_norm')) # Constraint function applied to the kernel matrix (see keras.constraints).
  bias_constraint: enumerate(('Constraint', 'MaxNorm', 'MinMaxNorm', 'NonNeg', 'RadialConstraint', 'UnitNorm','deserialize', 'get', 'max_norm', 'min_max_norm', 'non_neg', 'radial_constraint', 'serialize', 'unit_norm')) # Constraint function applied to the bias vector (see keras.constraints).
keras_layers_Conv3DTranspose:
  strides: (1,1,1) # An integer or tuple/list of 3 integers, specifying the strides of the convolution along the depth, height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1.
  padding: enumerate(('valid', 'same')) # one of "valid" or "same" (case-insensitive). "valid" means no padding. "same" results in padding with zeros evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input.
  output_padding: (1,1,1) # An integer or tuple/list of 3 integers, specifying the amount of padding along the depth, height, and width. Can be a single integer to specify the same value for all spatial dimensions. The amount of output padding along a given dimension must be lower than the stride along that same dimension. If set to None (default), the output shape is inferred.
  data_format: '' # A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch_size, depth, height, width, channels) while channels_first corresponds to inputs with shape (batch_size, channels, depth, height, width). It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json. If you never set it, then it will be "channels_last".
  dilation_rate: (1,1,1) # an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1.
  activation: enumerate(('deserialize', 'elu', 'exponential', 'get', 'hard_sigmoid', 'linear', 'relu', 'selu', 'serialize', 'sigmoid', 'softmax', 'softplus', 'softsign', 'swish', 'tanh')) # Activation function to use. If you don't specify anything, no activation is applied ( see keras.activations).
  use_bias: False # Boolean, whether the layer uses a bias vector.
  kernel_initializer: enumerate(('Constant', 'GlorotNormal', 'GlorotUniform', 'HeNormal', 'HeUniform', 'Identity', 'Initializer', 'LecunNormal', 'LecunUniform', 'Ones', 'Orthogonal', 'RandomNormal', 'RandomUniform', 'TruncatedNormal', 'VarianceScaling', 'Zeros', 'constant', 'deserialize', 'get', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform', 'identity', 'lecun_normal', 'lecun_uniform', 'ones', 'orthogonal', 'random_normal', 'random_uniform', 'serialize', 'truncated_normal', 'variance_scaling', 'zeros')) # Initializer for the kernel weights matrix ( see keras.initializers). Defaults to 'glorot_uniform'.
  bias_initializer: enumerate(('Constant', 'GlorotNormal', 'GlorotUniform', 'HeNormal', 'HeUniform', 'Identity', 'Initializer', 'LecunNormal', 'LecunUniform', 'Ones', 'Orthogonal', 'RandomNormal', 'RandomUniform', 'TruncatedNormal', 'VarianceScaling', 'Zeros', 'constant', 'deserialize', 'get', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform', 'identity', 'lecun_normal', 'lecun_uniform', 'ones', 'orthogonal', 'random_normal', 'random_uniform', 'serialize', 'truncated_normal', 'variance_scaling', 'zeros')) # Initializer for the bias vector ( see keras.initializers). Defaults to 'zeros'.
  kernel_regularizer: enumerate(('L1', 'L1L2', 'L2', 'Regularizer', 'get', 'l1', 'l1_l2', 'l2', 'serialize')) # Regularizer function applied to the kernel weights matrix ( see keras.regularizers).
  bias_regularizer: enumerate(('L1', 'L1L2', 'L2', 'Regularizer', 'get', 'l1', 'l1_l2', 'l2', 'serialize')) # Regularizer function applied to the bias vector ( see keras.regularizers).
  activity_regularizer: enumerate(('L1', 'L1L2', 'L2', 'Regularizer', 'get', 'l1', 'l1_l2', 'l2', 'serialize')) # Regularizer function applied to the output of the layer (its "activation") (see keras.regularizers).
  kernel_constraint: enumerate(('Constraint', 'MaxNorm', 'MinMaxNorm', 'NonNeg', 'RadialConstraint', 'UnitNorm','deserialize', 'get', 'max_norm', 'min_max_norm', 'non_neg', 'radial_constraint', 'serialize', 'unit_norm')) # Constraint function applied to the kernel matrix ( see keras.constraints). 
  bias_constraint: enumerate(('Constraint', 'MaxNorm', 'MinMaxNorm', 'NonNeg', 'RadialConstraint', 'UnitNorm','deserialize', 'get', 'max_norm', 'min_max_norm', 'non_neg', 'radial_constraint', 'serialize', 'unit_norm')) # Constraint function applied to the bias vector ( see keras.constraints).
keras_layers_Dense:
  activation: 'relu' # Activation function to use. If you don't specify anything, no activation is applied (ie. "linear" activation: a(x) = x).
  use_bias: True # Boolean, whether the layer uses a bias vector.
  kernel_initializer: "glorot_uniform" # Initializer for the kernel weights matrix.
  bias_initializer: "zeros" # Initializer for the bias vector.
  kernel_regularizer: None # Regularizer function applied to the kernel weights matrix.
  bias_regularizer: None # Regularizer function applied to the bias vector.
  activity_regularizer: None # Regularizer function applied to the output of the layer (its "activation").
  kernel_constraint: None # Constraint function applied to the kernel weights matrix.
  bias_constraint: None # Constraint function applied to the bias vector.
keras_layers_MaxPooling3D:
  strides: (1,1,1) # tuple of 3 integers, or None. Strides values.
  padding: enumerate(('valid', 'same')) # One of "valid" or "same" (case-insensitive). "valid" means no padding. "same" results in padding evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input.
  data_format: enumerate(('channels_first', 'channels_last')) # A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3). It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json. If you never set it, then it will be "channels_last".
keras_layers_AveragePooling3D:
  strides: (1,1,1) # tuple of 3 integers, or None. Strides values.
  padding: enumerate(('valid', 'same')) # One of "valid" or "same" (case-insensitive). "valid" means no padding. "same" results in padding evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input.
  data_format: enumerate(('channels_first', 'channels_last')) # A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3). It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json. If you never set it, then it will be "channels_last".
keras_layers_ZeroPadding3D:
  data_format: enumerate(('channels_first', 'channels_last')) # A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3). It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json. If you never set it, then it will be "channels_last".
keras_layers_BatchNormalization:
  axis: -1 # Integer, the axis that should be normalized (typically the features axis). For instance, after a Conv2D layer with data_format="channels_first", set axis=1 in BatchNormalization. 
  momentum: 0.99 # Momentum for the moving average.
  epsilon: 0.001 # 	Small float added to variance to avoid dividing by zero.
  center: True # If True, add offset of beta to normalized tensor. If False, beta is ignored.
  scale: True #	If True, multiply by gamma. If False, gamma is not used. When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer.
  beta_initializer: 'zeros' # Initializer for the beta weight.
  gamma_initializer: 'ones' # Initializer for the gamma weight.
  moving_mean_initializer: 'zeros' # Initializer for the moving mean.
  moving_variance_initializer: 'ones' # Initializer for the moving variance.
  beta_regularizer: '' # Optional regularizer for the beta weight.
  gamma_regularizer: '' # Optional regularizer for the gamma weight.
  beta_constraint: '' # Optional constraint for the beta weight.
  gamma_constraint: '' # Optional constraint for the gamma weight.
  training: False # Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). Whether to return the output in training mode (normalized with statistics of the current batch) or in inference mode (normalized with moving statistics). NOTE: make sure to set this parameter correctly, or else your training/inference will not work properly. 
  trainable: True # Boolean, if True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). 
  name: '' # String, the name of the layer.
  reuse: False # Boolean, whether to reuse the weights of a previous layer by the same name. 
  renorm: False # Whether to use Batch Renormalization (Ioffe, 2017). This adds extra variables during training. The inference is the same for either value of this parameter. 
  renorm_clipping: {} # A dictionary that may map keys 'rmax', 'rmin', 'dmax' to scalar Tensors used to clip the renorm correction. The correction (r, d) is used as corrected_value = normalized_value * r + d, with r clipped to [rmin, rmax], and d to [-dmax, dmax]. Missing rmax, rmin, dmax are set to inf, 0, inf, respectively. 
  renorm_momentum: 0.99 # Momentum used to update the moving means and standard deviations with renorm. Unlike momentum, this affects training and should be neither too small (which would add noise) nor too large (which would give stale estimates). Note that momentum is still applied to get the means and variances for inference. 
  fused: False # if None or True, use a faster, fused implementation if possible. If False, use the system recommended implementation. 
  virtual_batch_size: 1 # An int. By default, virtual_batch_size is None, which means batch normalization is performed across the whole batch. When virtual_batch_size is not None, instead perform "Ghost Batch Normalization", which creates virtual sub-batches which are each normalized separately (with shared gamma, beta, and moving statistics). Must divide the actual batch size during execution. 
  adjustment: '' # A function taking the Tensor containing the (dynamic) shape of the input tensor and returning a pair (scale, bias) to apply to the normalized values (before gamma and beta), only during training. For example, if axis==-1, adjustment = lambda shape: ( tf.random.uniform(shape[-1:], 0.93, 1.07), tf.random.uniform(shape[-1:], -0.1, 0.1)) will scale the normalized value by up to 7% up or down, then shift the result by up to 0.1 (with independent scaling and bias for each feature but shared across all examples), and finally apply gamma and/or beta. If None, no adjustment is applied. Cannot be specified if virtual_batch_size is specified.
keras_layers_UpSampling3D:
  size: (2,2,2) # Int, or tuple of 3 integers. The upsampling factors for dim1, dim2 and dim3.
  data_format: '' # A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3). It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json. If you never set it, then it will be "channels_last".
keras_models_Model:
  name: '' # String, the name of the model.
keras_models_Model_compile:
  optimizer: enumerate(('Adadelta', 'Adagrad', 'Adam', 'Adamax', 'Ftrl', 'Nadam', 'Optimizer', 'RMSprop', 'SGD', 'deserialize', 'get', 'schedules', 'serialize')) # String (name of optimizer) or optimizer instance. See tf.keras.optimizers. 
  loss: enumerate(('BinaryCrossentropy', 'CategoricalCrossentropy', 'CategoricalHinge', 'CosineSimilarity', 'Hinge', 'Huber', 'KLD', 'KLDivergence', 'LogCosh', 'Loss', 'MAE', 'MAPE', 'MSE', 'MSLE', 'MeanAbsoluteError', 'MeanAbsolutePercentageError', 'MeanSquaredError', 'MeanSquaredLogarithmicError', 'Poisson', 'Reduction', 'SparseCategoricalCrossentropy', 'SquaredHinge', 'binary_crossentropy', 'categorical_crossentropy', 'categorical_hinge', 'cosine_similarity', 'deserialize', 'get', 'hinge', 'huber', 'kl_divergence', 'kld', 'kullback_leibler_divergence', 'log_cosh', 'logcosh', 'mae', 'mape', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_error', 'mean_squared_logarithmic_error', 'mse', 'msle', 'poisson', 'serialize', 'sparse_categorical_crossentropy', 'squared_hinge')) # Loss function. Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses. A loss function is any callable with the signature loss = fn(y_true, y_pred), where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1)). y_pred should have shape (batch_size, d0, .. dN). The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None, return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. 
  metrics: enumerate(('AUC', 'Accuracy', 'BinaryAccuracy', 'BinaryCrossentropy', 'CategoricalAccuracy', 'CategoricalCrossentropy', 'CategoricalHinge', 'CosineSimilarity', 'FalseNegatives', 'FalsePositives', 'Hinge', 'KLD', 'KLDivergence', 'LogCoshError', 'MAE', 'MAPE', 'MSE', 'MSLE', 'Mean', 'MeanAbsoluteError', 'MeanAbsolutePercentageError', 'MeanIoU', 'MeanRelativeError', 'MeanSquaredError', 'MeanSquaredLogarithmicError', 'MeanTensor', 'Metric', 'Poisson', 'Precision', 'PrecisionAtRecall', 'Recall', 'RecallAtPrecision', 'RootMeanSquaredError', 'SensitivityAtSpecificity', 'SparseCategoricalAccuracy', 'SparseCategoricalCrossentropy', 'SparseTopKCategoricalAccuracy', 'SpecificityAtSensitivity', 'SquaredHinge', 'Sum', 'TopKCategoricalAccuracy', 'TrueNegatives', 'TruePositives', 'binary_accuracy', 'binary_crossentropy', 'categorical_accuracy', 'categorical_crossentropy', 'deserialize', 'get', 'hinge', 'kl_divergence', 'kld', 'kullback_leibler_divergence', 'mae', 'mape', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_error', 'mean_squared_logarithmic_error', 'mse', 'msle', 'poisson', 'serialize', 'sparse_categorical_accuracy', 'sparse_categorical_crossentropy', 'sparse_top_k_categorical_accuracy', 'squared_hinge', 'top_k_categorical_accuracy')) # List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics. Typically you will use metrics=['accuracy']. A function is any callable with the signature result = fn(y_true, y_pred). To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}. You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']]. When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy, tf.keras.metrics.CategoricalAccuracy, tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. 
  loss_weights: [0.0] # Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. 
  weighted_metrics: [0.0] # List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. 
  run_eagerly: False # Bool. Defaults to False. If True, this Model's logic will not be wrapped in a tf.function. Recommended to leave this as None unless your Model cannot be run inside a tf.function. run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy. 
  steps_per_execution: 1 # Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N, Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). 
keras_models_Model_fit:
  batch_size: 1 # Integer or None. Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches).
  epochs: 50 # Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch, epochs is to be understood as "final epoch". The model is not trained for a number of iterations given by epochs, but merely until the epoch of index epochs is reached.
  verbose: 1 # 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment).
  callbacks: '' # List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks. Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit. tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit. Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy, and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value.
  validation_split: 0.0 # Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy.
  shuffle: False # Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None.
  class_weight: '' # Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to "pay more attention" to samples from an under-represented class.
  sample_weight: '' # Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x.
  initial_epoch: 0 # Integer. Epoch at which to start training (useful for resuming a previous training run).
  steps_per_epoch: 0 # Integer or None. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy: * steps_per_epoch=None is not supported.
  validation_steps: '' # Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time.
  validation_batch_size: 0 # Integer or None. Number of samples per validation batch. If unspecified, will default to batch_size. Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches).
  validation_freq: '' # Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs.
  max_queue_size: 10 # Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10.
  workers: 1 # Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1.
  use_multiprocessing: False # Boolean. Used for generator or keras.utils.Sequence input only. If True, use process-based threading. If unspecified, use_multiprocessing will default to False. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes.
keras_models_Model_predict:
  steps: 1 # Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None. If x is a tf.data dataset and steps is None, predict will run until the input dataset is exhausted.
  callbacks: '' # List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks.
  max_queue_size: 10 # Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10.
  workers: 1 # Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1.
  use_multiprocessing: False # Boolean. Used for generator or keras.utils.Sequence input only. If True, use process-based threading. If unspecified, use_multiprocessing will default to False. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes.
keras_models_Model_summary:
  line_length: 0 # Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes).
  positions: [.33, .55, .67, 1.] # Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.].
  print_fn: '' # Print function to use. Defaults to print. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary.
keras_models_Model_load_model:
  custom_objects: {'dice_coef_loss': dice_coef_loss, 'dice_coef': dice_coef} # Optional dictionary mapping names (strings) to custom classes or functions to be considered during deserialization.
  compile: True # Boolean, whether to compile the model after loading.
  options: '' # Optional tf.saved_model.LoadOptions object that specifies options for loading from SavedModel. 
keras_optimizers_Adam:
  beta_1: 0.9 # A float value or a constant float tensor, or a callable that takes no arguments and returns the actual value to use. The exponential decay rate for the 1st moment estimates. Defaults to 0.9.
  beta_2: 0.999 # A float value or a constant float tensor, or a callable that takes no arguments and returns the actual value to use, The exponential decay rate for the 2nd moment estimates. Defaults to 0.999.
  epsilon: 1e-7 # A small constant for numerical stability. This epsilon is "epsilon hat" in the Kingma and Ba paper (in the formula just before Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to 1e-7.
  amsgrad: False # Boolean. Whether to apply AMSGrad variant of this algorithm from the paper "On the Convergence of Adam and beyond". Defaults to False.
  name: 'Adam' # Optional name for the operations created when applying gradients. Defaults to "Adam".  
keras_utils_to_categorical:
  num_classes: 1  # Total number of classes. If None, this would be inferred as max(y) + 1.
  dtype : enumerate(('bfloat16', 'bool', 'complex128', 'complex64', 'double',  'float16',  'float32',  'float64',  'half',  'int8',  'int16',  'int32',  'int64',  'qint8',  'qint16',  'qint32',  'quint8',  'quint16',  'resource',  'string',  'uint8',  'uint16',  'uint32',  'uint64',  'variant')) # The data type expected by the input. Default: 'float32'.   
   
      